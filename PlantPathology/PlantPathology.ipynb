{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PlantPathology.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyM3y21EWJeMwZ1AohkFoz3z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wR51yhymXyNn","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLnvgk1WPqXY","colab_type":"code","colab":{}},"source":["!free -h"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Boa_t11VZbYw","colab_type":"code","colab":{}},"source":["import os\n","print(os.getcwd())\n","os.chdir('/content/drive/My Drive/Google Colab/kaggle/PlantPathology')\n","print(os.getcwd())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPfyh4MZZmBy","colab_type":"code","colab":{}},"source":["# !unzip plant-pathology-2020-fgvc7.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPy3M2hxg-qw","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from os.path import join\n","from glob import glob\n","from itertools import chain\n","from PIL import Image\n","from torchvision import datasets, models, transforms\n","import torch.utils.data as data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cli4dniLlMjF","colab_type":"code","colab":{}},"source":["_data = pd.read_csv(\"train.csv\")\n","\n","healthy = (_data['healthy'][_data['healthy'] == 1])\n","multiple_diseases = (_data['multiple_diseases'][_data['multiple_diseases'] == 1])\t\n","rust = (_data['rust'][_data['rust'] == 1])\n","scab = (_data['scab'][_data['scab'] == 1])\n","\n","print(\"healthy:\" , len(healthy))\n","print(\"multiple_diseases:\" , len(multiple_diseases))\n","print(\"rust:\" , len(rust))\n","print(\"scab:\", len(scab))\n","\n","TRAIN_HEALTHY_NUM = int(len(healthy) * 0.8)\n","print(TRAIN_HEALTHY_NUM)\n","\n","TRAIN_MULTIPLE_NUM = int(len(multiple_diseases) * 0.8)\n","print(TRAIN_MULTIPLE_NUM)\n","\n","TRAIN_RUST_NUM = int(len(rust) * 0.8)\n","print(TRAIN_RUST_NUM)\n","\n","TRAIN_SCAB_NUM = int(len(scab) * 0.8)\n","print(TRAIN_SCAB_NUM)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uys97i_dhyBH","colab_type":"code","colab":{}},"source":["class ImageTransform():\n","    def __init__(self, resize, mean, std):\n","        self.data_transform = {\n","            'train': transforms.Compose([\n","                transforms.RandomResizedCrop(\n","                    resize, scale=(0.5, 1.0)),  # データオーギュメンテーション\n","                transforms.RandomHorizontalFlip(),  # データオーギュメンテーション\n","                transforms.ToTensor(),  # テンソルに変換\n","                transforms.Normalize(mean, std)  # 標準化\n","            ]),\n","            'val': transforms.Compose([\n","                transforms.Resize(resize),  # リサイズ\n","                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\n","                transforms.ToTensor(),  # テンソルに変換\n","                transforms.Normalize(mean, std)  # 標準化\n","            ])\n","        }\n","\n","    def __call__(self, img, phase='train'):\n","        return self.data_transform[phase](img)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hN4cHS3lkLqc","colab_type":"code","colab":{}},"source":["def make_datapath_list(phase, class_data):\n","    TRAIN_MAX = {'healthy': TRAIN_HEALTHY_NUM, 'multiple_diseases': TRAIN_MULTIPLE_NUM, 'rust': TRAIN_RUST_NUM, 'scab': TRAIN_SCAB_NUM}\n","    train_list = []\n","    val_list = []\n","    class_names = ['healthy', 'multiple_diseases', 'rust', 'scab']\n","    count = {'healthy': 0, 'multiple_diseases': 0, 'rust': 0, 'scab': 0}\n","\n","    for i in range(0, 1821):\n","        path = join(\"images\", class_data.iloc[i, 0] + '.jpg')\n","\n","        for j in range(1, 5):\n","            if class_data.iloc[i, j] == 1:\n","                if TRAIN_MAX[class_names[j - 1]] >= count[class_names[j - 1]]:\n","                    train_list.append(path)\n","                else:\n","                    val_list.append(path)\n","                \n","                count[class_names[j - 1]] += 1\n","                break\n","\n","    \n","    return train_list, val_list\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mowJzv22jjOD","colab_type":"code","colab":{}},"source":["class LoadDataset(data.Dataset):\n","    \n","    def __init__(self, file_list, transform=None, phase='train', class_data=None):\n","        self.file_list = file_list  # ファイルパスのリスト\n","        self.transform = transform  # 前処理クラスのインスタンス\n","        self.phase = phase  # train or valの指定\n","        self.class_data = class_data\n","        \n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, index):\n","        # index番目の画像をロード\n","        img_path = self.file_list[index]\n","        img = Image.open(img_path).convert(\"RGB\")  # [高さ][幅][色RGB]\n","\n","        # 画像の前処理を実施\n","        img_transformed = self.transform(\n","            img, self.phase)  # torch.Size([3, 224, 224])\n","        \n","        row = int(img_path[13:-4])\n","        _class_data = self.class_data.iloc[row]\n","        for i in range(1, 5):\n","            if _class_data[i] == 1:\n","                label = i - 1\n","        \n","        # 画像のラベルをファイル名から抜き出す\n","        return img_transformed, label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ey6xxHLWj1WH","colab_type":"code","colab":{}},"source":["def make_dataLoader():\n","    \n","    size = 224\n","    mean = (0.485, 0.456, 0.406)\n","    std = (0.229, 0.224, 0.225)\n","    \n","    class_data = pd.read_csv(\"train.csv\")\n","\n","    batch_size = 32\n","    # trainとvalの画像へのパスを作成\n","    train_list, val_list = make_datapath_list(phase=\"val\", class_data=class_data)\n","\n","    # Datasetを作成する\n","    train_dataset = LoadDataset(\n","        file_list=train_list, transform=ImageTransform(size, mean, std), phase='train', class_data=class_data)\n","\n","    val_dataset = LoadDataset(\n","        file_list=val_list, transform=ImageTransform(size, mean, std), phase='val', class_data=class_data)\n","\n","\n","    # DataLoaderを作成する\n","    train_dataloader = data.DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    val_dataloader = data.DataLoader(\n","        val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # 辞書オブジェクトにまとめる\n","    dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n","\n","    return dataloaders_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"32QUz29JkoQD","colab_type":"code","colab":{}},"source":[" #モジュールのインポート\n","import random, yaml\n","import torch, torchvision\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from tqdm import tqdm\n","from random import seed, sample\n","from sklearn.metrics import classification_report\n","from torchvision import datasets, models, transforms\n","from collections import OrderedDict\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2C9yOJikzXq","colab_type":"code","colab":{}},"source":["def fine_tuning_VGG16():\n","    # VGG-16モデルのインスタンスを生成\n","    use_pretrained = True  # 学習済みのパラメータを使用\n","    net = models.vgg16(pretrained=use_pretrained)\n","\n","    net.classifier = nn.Sequential(OrderedDict([\n","        (\"0\", nn.Linear(25088, 4096)),\n","        (\"1\", nn.ReLU()),\n","        (\"2\", nn.Dropout(p=0.5)),\n","        (\"3\", nn.Linear(4096, 4))\n","    ]))\n","\n","\n","    # 訓練モードに設定\n","    net.train()\n","    print('ネットワーク設定完了：学習済みの重みをロードし、訓練モードに設定しました')\n","\n","    #以下fine turning\n","    params_to_update_1 = []\n","    params_to_update_2 = []\n","    params_to_update_3 = []\n","\n","    # 学習させる層のパラメータ名を指定\n","    update_param_names_1 = [\"features.21.weight\", \"features.21.bias\",\"features.24.weight\", \"features.24.bias\",\"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\"]\n","    update_param_names_2 = [\"classifier.0.weight\",\n","                            \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\"]\n","    # update_param_names_3 = [\"classifier.6.weight\", \"classifier.6.bias\"]\n","\n","    # パラメータごとに各リストに格納する\n","    for name, param in net.named_parameters():\n","        if name in update_param_names_1:\n","            param.requires_grad = True\n","            params_to_update_1.append(param)\n","            print(\"params_to_update_1に格納：\", name)\n","\n","        elif name in update_param_names_2:\n","            param.requires_grad = True\n","            params_to_update_2.append(param)\n","            print(\"params_to_update_2に格納：\", name)\n","\n","        # elif name in update_param_names_3:\n","        #     param.requires_grad = True\n","        #     params_to_update_3.append(param)\n","        #     print(\"params_to_update_3に格納：\", name)\n","\n","        else:\n","            param.requires_grad = False\n","            print(\"勾配計算なし。学習しない：\", name)\n","        \n","        print('fine_tuning設定完了')\n","        # Optimizer設定\n","    optimizer = optim.SGD([\n","        {'params': params_to_update_1, 'lr': 5e-4},\n","        {'params': params_to_update_2, 'lr': 5e-3},\n","        # {'params': params_to_update_3, 'lr': 5e-3}\n","    ], momentum=0.9)\n","\n","    return net,optimizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"soGCb_6mls9P","colab_type":"code","colab":{}},"source":["def do_train_return_dic(net, dataloader_dict, criterion, optimizer, num_epochs):\n","\n","    # 初期設定\n","    # GPUが使えるかを確認\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"使用デバイス：\", device)\n","\n","    # ネットワークをGPUへ\n","    net = net.to(device)\n","    if device == 'cuda':\n","        net = torch.nn.DataParallel(net) # make parallel\n","\n","        # ネットワークがある程度固定であれば、高速化させる\n","        torch.backends.cudnn.benchmark = True\n","\n","\n","    epoch_loss_dic = {}\n","    epoch_acc_dic = {}\n","\n","    # epochのループ\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-------------')\n","\n","        # epochごとの訓練と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train()  # モデルを訓練モードに\n","            else:\n","                net.eval()   # モデルを検証モードに\n","\n","            epoch_loss = 0.0  # epochの損失和\n","            epoch_corrects = 0  # epochの正解数\n","\n","            # 未学習時の検証性能を確かめるため、epoch=0の訓練は省略\n","            if (epoch == 0) and (phase == 'train'):\n","                continue\n","\n","            # データローダーからミニバッチを取り出すループ\n","            for inputs, labels in tqdm(dataloader_dict[phase]):\n","\n","                # GPUが使えるならGPUにデータを送る\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # optimizerを初期化\n","                optimizer.zero_grad()\n","\n","\n","                # 順伝搬（forward）計算\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(inputs)\n","                    loss = criterion(outputs, labels)  # 損失を計算\n","                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n","\n","                    # 訓練時はバックプロパゲーション\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                    # 結果の計算\n","                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n","                    # 正解数の合計を更新\n","                    epoch_corrects += torch.sum(preds == labels.data)\n","\n","            # epochごとのlossと正解率を表示\n","            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double(\n","            ) / len(dataloader_dict[phase].dataset)\n","\n","            epoch_loss_dic.setdefault(phase, []).append(epoch_loss)\n","            epoch_acc_dic.setdefault(phase, []).append(epoch_acc.to(\"cpu\").item())\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","    # PyTorchのネットワークパラメータの保存\n","    save_path = './weights_fine_tuning_drill.pth'\n","    torch.save(net.state_dict(), save_path)\n","    \n","    return epoch_loss_dic, epoch_acc_dic"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4jDiuHomFeP","colab_type":"code","colab":{}},"source":["def do_test(dataloaders_dict):\n","    net = models.vgg16(False)\n","\n","    net.classifier = nn.Sequential(OrderedDict([\n","        (\"0\", nn.Linear(25088, 4096)),\n","        (\"1\", nn.ReLU()),\n","        (\"2\", nn.Dropout(p=0.5)),\n","        (\"3\", nn.Linear(4096, 4))\n","    ]))\n","\n","    net_weights = torch.load('./weights_fine_tuning_drill.pth', map_location={'cuda': 'cpu'})\n","    new_state_dict = OrderedDict()\n","\n","    for k, v in net_weights.items():\n","        k = k[7:]\n","        new_state_dict[k]=v\n","\n","    net.load_state_dict(new_state_dict)\n","\n","    \n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"使用デバイス：\", device)\n","\n","    net = net.to(device)\n","    if device == 'cuda':\n","        net = torch.nn.DataParallel(net) # make parallel\n","\n","        # ネットワークがある程度固定であれば、高速化させる\n","        torch.backends.cudnn.benchmark = True\n","\n","    net.eval()\n","\n","    Y,pred = [], []\n","    for inputs, labels in tqdm(dataloaders_dict[\"val\"]):\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = net(inputs)\n","        _, preds = torch.max(outputs, 1)  # ラベルを予測\n","\n","        Y.extend(labels.to(\"cpu\").numpy().tolist())\n","        pred.extend(preds.to(\"cpu\").numpy().tolist())\n","\n","    print(classification_report(Y, pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6RlgcJ_uCiR","colab_type":"code","colab":{}},"source":[" def plot_history_loss(loss):\n","    # hist.historyに辞書型で損失値や精度が入っているので取得して表示\n","    plt.plot(loss['train'],label=\"loss for training\")\n","    plt.plot(loss['val'],label=\"loss for validation\")\n","    \n","    #matplotlibの細かい設定\n","    plt.title('model loss')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(loc='best')\n","    \n","    plt.show()\n","    \n","\n","def plot_history_acc(acc):\n","    plt.plot(acc['train'],label=\"acc for training\")\n","    plt.plot(acc['val'],label=\"acc for validation\")\n","    plt.title('model accuracy')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.legend(loc='best')\n","    plt.ylim([0, 1])\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpFNwZJgj24A","colab_type":"code","colab":{}},"source":["def main():\n","    dataloaders_dict = make_dataLoader() #trとvalのデータ整形\n","    \n","    net,optimizer = fine_tuning_VGG16() # VGGF16モデルを読み込む\n","    criterion = nn.CrossEntropyLoss()  # 損失関数の設定\n","    \n","    # 学習・検証を実行する\n","    num_epochs = 40\n","    epoch_loss_dic, epoch_acc_dic = do_train_return_dic(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n","    \n","    plot_history_loss(epoch_loss_dic)\n","\n","    plot_history_acc(epoch_acc_dic)\n","\n","    # 検証\n","    # do_test(dataloaders_dict)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnZD5J8LmeLM","colab_type":"code","colab":{}},"source":["if __name__=='__main__':\n","    # 乱数のシードを設定\n","    torch.manual_seed(1234)\n","    np.random.seed(1234)\n","    random.seed(1234)\n","    \n","    main()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUZQC6Xg_WF5","colab_type":"code","colab":{}},"source":["dataloaders_dict = make_dataLoader() #trとvalのデータ整形\n","\n","do_test(dataloaders_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dS4BTCzxmmVQ","colab_type":"code","colab":{}},"source":["test = pd.read_csv(\"test.csv\")\n","test_path_list = [join('images', path + '.jpg') for path in chain.from_iterable(test.values.tolist())]\n","\n","size = 224\n","mean = (0.485, 0.456, 0.406)\n","std = (0.229, 0.224, 0.225)\n","\n","test_transform = transforms.Compose([\n","                transforms.Resize(size),  # リサイズ\n","                transforms.CenterCrop(size),  # 画像中央をresize×resizeで切り取り\n","                transforms.ToTensor(),  # テンソルに変換\n","                transforms.Normalize(mean, std)  # 標準化\n","            ])\n","trained_net = models.vgg16(False)\n","\n","trained_net.classifier = nn.Sequential(OrderedDict([\n","    (\"0\", nn.Linear(25088, 4096)),\n","    (\"1\", nn.ReLU()),\n","    (\"2\", nn.Dropout(p=0.5)),\n","    (\"3\", nn.Linear(4096, 4))\n","]))\n","\n","net_weights = torch.load('./weights_fine_tuning_drill.pth', map_location={'cuda': 'cpu'})\n","from collections import OrderedDict\n","new_state_dict = OrderedDict()\n","\n","for k, v in net_weights.items():\n","    k = k[7:]\n","    new_state_dict[k]=v\n","\n","trained_net.load_state_dict(new_state_dict)\n","\n","\n","trained_net.eval()\n","\n","preds = []\n","# print(test_path_list)\n","# img = Image.open(test_path_list[0]).convert(\"RGB\")\n","# img_transformed = test_transform(img).unsqueeze(0)\n","# o = trained_net(img_transformed)\n","# pred = int(o.argmax())\n","# print(pred)\n","\n","for path in tqdm(test_path_list):\n","    img = Image.open(path).convert(\"RGB\")  # [高さ][幅][色RGB]\n","\n","    img_transformed = test_transform(img).unsqueeze(0)  # torch.Size([3, 224, 224])\n","    \n","    output = trained_net(img_transformed)\n","    preds += [int(output.argmax())]\n","\n","print('success')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfBbpQaANase","colab_type":"code","colab":{}},"source":["print(len(preds))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilg0xeEAAHQH","colab_type":"code","colab":{}},"source":["id_list = [p[7:-4] for p in test_path_list]\n","\n","pred_dict = {'healthy': [0] * 1821, 'multiple_diseases': [0] * 1821, 'rust': [0] * 1821, 'scab': [0] * 1821}\n","class_list = ['healthy', 'multiple_diseases', 'rust', 'scab']\n","\n","for i, pred in enumerate(preds):\n","    pred_dict[class_list[pred]][i] = 1\n","\n","submit_file = pd.DataFrame({'image_id':id_list, 'healthy': pred_dict['healthy'], 'multiple_diseases': pred_dict['multiple_diseases'], 'rust': pred_dict['rust'], 'scab': pred_dict['scab']})\n","submit_file.to_csv('submit.csv', index = False)"],"execution_count":0,"outputs":[]}]}